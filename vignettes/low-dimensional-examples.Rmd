---
title: "Low Dimensional Examples"
author: "Andy Iskauskas"
date: "14/05/2021"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Low Dimensional Examples}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---

```{r, include=FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
  )
```

```{r setup}
library(hmer)
library(lhs)
library(ggplot2)
```

# One-dimensional Example: Sine Function

## Setup

We first look at the simplest possible example: a single univariate function. We take the following sine function:

\(f(x) = 2x + 3x\sin\left(\frac{5\pi(x-0.1)}{0.4}\right).\)

This should demonstrate the main features of emulation and history matching over a couple of waves. The other advantage of using such a simple function (in this case as well as in the later, two-dimensional, case) is that the function can be evaluated quickly, so we can compare the emulator performance against the actual function value very easily. This is seldom the case in real-world applications, where a simulator is often a 'black box' function that can take a long time to evaluate at any given point.

```{r}
func <- function(x) {
  2*x + 3*x*sin(5*pi*(x-0.1)/0.4)
}
```

We presume that we want to emulate this function over the input space $x\in[0,0.6]$. To train an emulator to this function, we require a set of known points. We'll evaluate the function at equally spaced points along the range $[0, 0.5]$: ten points will be sufficient for training this one-dimensional emulator. A general rule of thumb is that we require a number of points equal to ten times the dimension of the input space.

```{r}
data1d <- data.frame(x = seq(0.05, 0.5, by = 0.05), f = func(seq(0.05, 0.5, by = 0.05)))
```

These points will be passed to the `hmer` package functions, in order for it to train an emulator to `func`, interpolate points between the data points above, and propose a set of new points for training a second-wave emulator.

## Emulator Training

To train the emulator, we require at least three things: the data to train on, the names of the outputs to emulate, and the parameter ranges. The function `emulator_from_data` can then be used to determine prior specifications for the emulator; namely expectations and variances of its component parts, as well as correlation lengths and other structural elements. It then performs Bayes Linear adjustment on this prior emulator to give us our trained emulator. To see these elements in more detail, consult the section "The Structure of a Bayes Linear Emulator" at the bottom of this document.

We therefore define the ranges of the parameters (in this case, just one parameter $x$) and use the `emulator_from_data` function, produced objects of class `Emulator`:

```{r}
ranges1d <- list(x = c(0, 0.6))
em1d <- emulator_from_data(data1d, c('f'), ranges1d)
em1d
```
The print statement for the emulator shows us the specifications: the basis functions chosen, the corresponding regression coefficients, the global variance, and the structure and hyperparameters of the correlation structure. We also note that the output of `emulator_from_data` is a named list of emulators: here, this is not particularly important as we only have one emulator.

The print statement also indicates that Bayes Linear adjustment has been applied: if we instead wanted to examine an unadjusted emulator, we could have run `emulator_from_data` with the option `adjusted = FALSE`, or having trained an emulator we can access the prior emulator by calling `o_em`. The below commands give the same output.

```{r}
emulator_from_data(data1d, c('f'), ranges1d, adjusted = FALSE)$f
em1d$f$o_em
```
The trained emulator, by virtue of having been provided the training points, 'knows' the value of the function at those points. Hence, the expectation of the emulator at those points is identical (up to numerical precision) to the known values of $f(x)$, and the variance at those points is $0$. We can access this information using the built-in functions `get_exp` and `get_cov` of the `Emulator` object.

```{r}
em1d$f$get_exp(data1d) - data1d$f
em1d$f$get_cov(data1d)
```
We can now use this trained emulator to evaluate the function at any point in the parameter range. While it will not exactly match the function value, each evaluation comes with its associated uncertainty. We define a 'large' set of additional points to evaluate the emulator on, and find their expectation and variance.

```{r}
test1d <- data.frame(x = seq(0, 0.6, by = 0.001))
em1d_exp <- em1d$f$get_exp(test1d)
em1d_var <- em1d$f$get_cov(test1d)
```

Since, by design, we have a function that is quick and easy to evaluate, we can directly compare the emulator predictions against the true function value, plotting the relevant quantities. In higher-dimensional cases, the `hmer` package has built-in plotting functionality, but here we use simple R plotting methods.

```{r fig.width = 7, fig.height = 7}
#> Define a data.frame for the plotting
plot1d <- data.frame(
  x = test1d$x,
  f = func(test1d$x),
  E = em1d_exp,
  max = em1d_exp + 3*sqrt(abs(em1d_var)),
  min = em1d_exp - 3*sqrt(abs(em1d_var))
)
plot(data = plot1d, f ~ x, ylim = c(min(plot1d[,-1]), max(plot1d[-1])),
     type = 'l', main = "Emulation of a Simple 1d Function", xlab = "x", ylab = "f(x)")
lines(data = plot1d, E ~ x, col = 'blue')
lines(data = plot1d, max ~ x, col = 'red', lty = 2)
lines(data = plot1d, min ~ x, col = 'red', lty = 2)
points(data = data1d, f ~ x, pch = 16, cex = 1)
legend('bottomleft', inset = c(0.05, 0.05), legend = c("Function value", "Emulated value", "Uncertainty Bounds"),
       col = c('black', 'blue', 'red'), lty = c(1,1,2))
```

We can see a few things from this plot. The emulator does exactly replicate the function at the points used for training (the black dots in the plots), and the corresponding uncertainty is zero at these points. Away from these points, the emulator does a good job of interpolating the function values, represented by the coincidence of the black and blue lines: the exception to this is areas far from a training point (the edges of the plot). However, we can see that even where the emulator expectation diverges from the function value, the both lines lie inside the uncertainty bounds (here demonstrated by the red lines).

## History Matching

We have a trained emulator, which we can see performs well over the region of interest. Suppose we want to find input points which result in a given output value. While with this function it would be straightforward to find such input points (either analytically or numerically), in general this would not be the case. We can therefore follow the history matching approach, using the emulator as a surrogate for the function.

History Matching consists of the following steps:
- Train emulators on known points in the target space;
- Use the trained emulators to rule out regions of parameter space that definitely cannot give rise to the desired output value;
- Sample a new set of points from the remaining space (the 'non-implausible' region);
- Input these new points into the model/simulator/function to obtain a new training set.

These four steps are repeated until either 1) we have a suitably large number of points producing the desired output; 2) the whole parameter space has been ruled out; 3) The emulators are as confident evaluating the whole parameter space as the model itself.

Here, we will not worry about these stopping conditions and instead perform the steps to complete two waves of emulation and history matching. The first thing we require is a target output value: suppose we want to find points $x$ such that $f(x)=0$, up to some uncertainty. We define this as follows:

```{r}
target1d <- list(f = list(val = 0, sigma = 0.05))
```

The `generate_new_runs` function is used to propose new points. There are a multitude of different methods that can be used to propose the new points: in this particular one-dimensional case, it makes sense to take the most basic approach. This is to generate a large number of space-filling points, reject those that the emulator rules out as implausible, and select the subset of the remaining points that has the maximal minimum distance between them (so as to cover as much of the non-implausible space as possible).

```{r}
new_points1d <- generate_new_runs(em1d, 10, target1d, method = 'lhs')
```

The necessary parameters here are the (list of) emulators, the number of points (here, 10), and the target(s).

Having obtained these new points, we include them on our previous plot, along with the target bounds, to demonstrate the logic of the point proposal.

```{r fig.width = 7, fig.height = 7}
plot(data = plot1d, f ~ x, ylim = c(min(plot1d[,-1]), max(plot1d[-1])),
     type = 'l', main = "Emulation of a Simple 1d Function", xlab = "x", ylab = "f(x)")
lines(data = plot1d, E ~ x, col = 'blue')
lines(data = plot1d, max ~ x, col = 'red', lty = 2)
lines(data = plot1d, min ~ x, col = 'red', lty = 2)
points(data = data1d, f ~ x, pch = 16, cex = 1)
abline(h = target1d$f$val, lty = 2)
abline(h = target1d$f$val + 3*target1d$f$sigma, lty = 2)
abline(h = target1d$f$val - 3*target1d$f$sigma, lty = 2)
points(unlist(new_points1d, use.names = F), y = func(unlist(new_points1d, use.names = F)), pch = 16, col = 'blue')
legend('bottomleft', inset = c(0.05, 0.05), legend = c("Function value", "Emulated value", "Uncertainty Bounds"),
       col = c('black', 'blue', 'red'), lty = c(1,1,2))
```

Note that some of the newly proposed points (in blue) do not live inside the target bounds - in particular, there are points on the far right that are not near the target. This is a consequence of the way that the emulator proposes points: because the emulator uncertainty is large in that region of the parameter space, it cannot rule out those regions with certainty, and therefore samples from that region. This is in contrast to many optimisation methods, which look for regions that satisfy the conditions: the history matching iteratively removes regions that cannot satisfy the conditions. Because of the built-in understanding of uncertainty, the history matching is guaranteed not to remove parts of parameter space that could result in an adequate match to the targets. The points in the high uncertainty region will be extremely instructive in training a second wave of emulators, which may consequently remove the space.

## Second Wave

The second wave is very similar to the first, so little needs to be explained.

```{r fig.width = 7, fig.height = 7}
new_data1d <- data.frame(x = unlist(new_points1d, use.names = F), f = func(unlist(new_points1d, use.names = F)))

em1d_2 <- emulator_from_data(new_data1d, c('f'), ranges1d)

plot1d_2 <- data.frame(
  x = plot1d$x, f = plot1d$f,
  E = em1d_2$f$get_exp(test1d),
  min = em1d_2$f$get_exp(test1d) - 3*sqrt(abs(em1d_2$f$get_cov(test1d))),
  max = em1d_2$f$get_exp(test1d) + 3*sqrt(abs(em1d_2$f$get_cov(test1d)))
)
plot(data = plot1d_2, f ~ x, ylim = c(min(plot1d_2[,-1]), max(plot1d_2[,-1])),
     type = 'l', main = "Emulator of a Simple 1-dimensional Function: Wave 2", xlab = "Parameter value", ylab = "Function value")
lines(data = plot1d_2, E ~ x, col = 'blue')
lines(data = plot1d_2, max ~ x, col = 'red', lty = 2)
lines(data = plot1d_2, min ~ x, col = 'red', lty = 2)
points(data = new_data1d, f ~ x, pch = 16, cex = 1)
legend('topleft', inset = c(0.05, 0.05), legend = c("Function value", "Emulated value", "Uncertainty Bounds"), col = c('black', 'blue', 'red'), lty = c(1,1,2))
```

This plot underlines the importance of using all waves of emulation. The first wave was trained over most of the space, and so gives a moderately confident estimate of the function on the interval $[0, 0.5]$. The second wave emulator is trained only on regions that are of interest at this point, and so is less confident than the first wave emulator in parts of parameter space. However, we still have the emulator from the first wave, which contains all the information of the previous training runs by design. In this wave of history matching, therefore, we use both the first- and second-wave emulator to propose points.

```{r, fig.width = 7, fig.height = 7}
new_new_points1d <- generate_new_runs(c(em1d_2, em1d), 10, z = target1d, method = 'lhs')

plot(data = plot1d_2, f ~ x, ylim = c(min(plot1d_2[,-1]), max(plot1d_2[,-1])),
     type = 'l', main = "Emulator of a Simple 1-dimensional Function: Wave 2", xlab = "Parameter value", ylab = "Function value")
lines(data = plot1d_2, E ~ x, col = 'blue')
lines(data = plot1d_2, max ~ x, col = 'red', lty = 2)
lines(data = plot1d_2, min ~ x, col = 'red', lty = 2)
points(data = new_data1d, f ~ x, pch = 16, cex = 1)
legend('topleft', inset = c(0.05, 0.05), legend = c("Function value", "Emulated value (wave 2)", "Uncertainty Bounds"), col = c('black', 'blue', 'red'), lty = c(1,1,2))
abline(h = target1d$f$val, lty = 2)
abline(h = target1d$f$val + 3*target1d$f$sigma, lty = 2)
abline(h = target1d$f$val - 3*target1d$f$sigma, lty = 2)
points(x = unlist(new_new_points1d, use.names = F), y = func(unlist(new_new_points1d, use.names = F)), pch = 16, col = 'blue')
```

We can see that the proposed points are much better overall than the first wave points. Because the second-wave emulator has much greater certainty in the region $[0.5, 0.6]$, it is far better at determining suitability of points in that region. From the plot, it looks like there should be points proposed from the central regions where the wave-two emulator is uncertain, but the fact that we are also using the first-wave emulator as a metric for point proposal means that these points are ruled out anyway.

# Two-dimensional example
We now consider a slightly higher-dimensional example, in order to consider emulator diagnostics and some more interesting plots. Consider the following pair of functions in two dimensions:

\(f_1(x) = 2\cos(1.2x-2) + 3\sin(-0.8y+1)\)
\(f_2(x) = y\sin x - 3\cos(xy)\)

The associated parameter space to explore is given by $x\in[-\pi/2, \pi/2]$ and $y\in[-1,1]$. We create a set of initial data to train emulators on: here we select the initial points to be space-filling using a Latin Hypercube design (package `lhs`):

```{r}
func1 <- function(x) {
  2*cos(1.2*x[[1]]-2) + 3*sin(-0.8*x[[2]]+1)
}
func2 <- function(x) {
  x[[2]]*sin(x[[1]]) - 3*cos(x[[1]]*x[[2]])
}
initial_data <- setNames(data.frame(sweep(maximinLHS(20, 2) - 1/2, 2, c(pi, 2), "*")), c('x', 'y'))
validation_data <- setNames(data.frame(sweep(maximinLHS(20, 2) - 1/2, 2, c(pi, 2), "*")), c('x', 'y'))
initial_data$f1 <- apply(initial_data, 1, func1)
initial_data$f2 <- apply(initial_data, 1, func2)
validation_data$f1 <- apply(validation_data, 1, func1)
validation_data$f2 <- apply(validation_data, 1, func2)
```

In this example, we have created two data sets: a training set `initial_data` and a validation set `validation_data`. This will allow us to see the performance of the emulators on previously untested points. In the one-dimensional case, this was not necessary as we can quite easily visualise the performance of the emulator using the simple plots.

As in the one-dimensional case, we will define a target to match to. In this case, we'll match to the value given when $x=0.1, y=0.4$ - this means that for this example we know that the target can be matched to, and gives us a check that the emulators are not ruling out a region of parameter space that they shouldn't be.

```{r}
ranges2d <- list(x = c(-pi/2, pi/2), y = c(-1, 1))
targets2d <- list(
  f1 = list(val = func1(c(0.1, 0.4)), sigma = 0.1),
  f2 = list(val = func2(c(0.1, 0.4)), sigma = 0.005)
)
```

We have placed a much tighter bound of $f_2$, suggesting that this target should be harder to hit. We construct some emulators as in the one-dimensional case:

```{r}
ems2d <- emulator_from_data(initial_data, c('f1','f2'), ranges2d)
ems2d
```
We obtain a list of trained emulators. The chosen specifications are interesting (in particular the active variable selection in $f_2$), but do not merit discussion at this point.

We want to check that the emulators are suitable for proposing new points robustly. We will check the following things:
- We want the emulator prediction to 'match' the model prediction; by which we wouldn't expect the emulator prediction to be more than $3\sigma$ away from the model prediction in the regions that matter.
- In general, we want the standardised errors of the emulator predictions relative to the model predictions to be not too large, but not too small. Large errors imply that the emulators are doing a bad job of fitting to validation points; consistently small errors imply that the emulators are underconfident and thus makes the wave of emulation less valuable for cutting out space.
- We want to ensure that the emulators do not rule out any regions of space that the model would not. The converse is okay, and indeed expected (the emulators are going to be more conservative about ruling out points), but the emulators should absolutely not rule out regions of space that may be valid.

These three tests are contained within `validation_diagnostics`, which prints out the corresponding plots for each emulator and returns any points which fail one or more of these checks.

```{r fig.width = 7, fig.height = 7}
invalid_points <- validation_diagnostics(ems2d, targets2d, validation_data)
```

Looking at these plots shows that the emulators are doing fine. Any failing points are flagged as red in the first two columns of the plots, and appear in the data.frame `invalid_points`: should we have such points it is useful to consider why they might be problematic (for example, a point at an edge or a corner of the parameter space can have strange behaviours). At this stage, we can make a variety of changes to the prior specifications in order to combat these issues, by utilising the `set_sigma` and `set_hyperparams` functions of the `Emulator` object. For instance, to inflate the emulator variance for the $f_2$ emulator:

```{r fig.width = 7, fig.height = 7}
inflated_em <- ems2d$f2$mult_sigma(2)
new_invalid <- validation_diagnostics(list(f1 = ems2d$f1, f2 = inflated_em), targets2d, validation_data)
```

The `set_sigma` and `set_hyperparams` functions create a new Emulator object, so in general we would replace the old emulator with the new one rather than combining lists (e.g. `ems2d$f2 <- emd2d$f2$set_sigma(...)`). In this case, such tinkering is not required and hence we did not replace the $f_2$ emulator in situ.

## Emulator Plots

The `hmer` package allows us to create plots analogous to those made by-hand in the one-dimensional case. The corresponding plots are contour plots over the input region.

```{r fig.width = 7, fig.height = 7}
emulator_plot(ems2d)
emulator_plot(ems2d, var_name = 'sd')
```

The `emulator_plot` function can produce a number of things: the default is an expectation plot equivalent to the blue line in the one dimensional case. The `var_name` command determines what is plotted: 'var' has it plot the variance and 'sd' the standard deviation, which play the part of the distance from blue line to red dotted lines in the one-dimensional case. These combined plots are used mainly for diagnostics - to obtain an expectation or variance plot with meaningful scale, we plot them one-by-one.

```{r fig.width = 7, fig.height = 7}
emulator_plot(ems2d$f1, plot_type = 'sd') + geom_point(data = initial_data, aes(x = x, y = y))
```

The above single plot shows that perhaps the emulator is overly uncertain at a small distance from its training points. This could be rectified by more carefully choosing the design so that points lie closer to the boundary, or by inflating the correlation length so that more distant points have a higher correlation with the training points. In any event, an underconfident emulator is not a disasterous outcome: it merely means that the points proposed will be from a larger non-implausible space.

The final two plots that `emulator_plot` can provide are those of implausibility, and nth-maximum implausibility.

```{r fig.width = 7, fig.height = 7}
emulator_plot(ems2d, plot_type = 'imp', targets = targets2d)
emulator_plot(ems2d, plot_type = 'nimp', targets = targets2d)
```

The implausibility of a point $x$, given a target $z$, is determined by the following expression:

\(I(x)^2 = \frac{(\mathbb{E}[f(x)]-z)^2}{\text{Var}[f(x)] + \sigma^2}.\)

Here $\mathbb{E}[f(x)]$ is the emulator expectation at the point, $\text{Var}[f(x)]$ is the emulator variance, and $\sigma^2$ corresponds to all other sources of uncertainty outside of the emulators (e.g. observation error, model discrepancy, ...). The smaller the implausibility at a given point, the more likely an emulator is to propose it as a new point. We can note that there are two reasons that implausibility might be small: either the numerator is small (in which case the difference between the emulator prediction and the target value is small, implying a 'good' point) or the denominator is large (in which case the point is in a region of parameter space that the emulator is uncertain about). Both are useful: the first for obvious reasons; the second because proposing points in such regions will make subsequent waves of emulators more certain about that region and thus allow us to reduce the allowed parameter space. The first plot here shows implausibility for each of the outputs.

Of course, we want proposed points to be acceptable to matching both $f_1$ and $f_2$. The second plot gives maximum implausibility, which is exactly as it sounds: the largest implausibility across all outputs at a point. We can see that the $f_1$ emulator drives much of the space reduction in the lower-right and upper-left quadrant of the region, while the $f_2$ emulator drives space reduction in the lower-left and (to some extent) upper-right. This maximum implausibility is equivalent to the determination made by the one-dimensional emulator, where in that case the non-implausible region was easy to see graphically.

We can now generate new points for a second wave of emulation.

```{r fig.width = 7, fig.height = 7}
new_points2d <- generate_new_runs(ems2d, 40, targets2d)
new_data2d <- data.frame(x = new_points2d$x, y = new_points2d$y,
                         f1 = apply(new_points2d, 1, func1), f2 = apply(new_points2d, 1, func2))
plot(new_data2d[,c('x','y')], pch = 16, cex = 0.5, xlim = c(-pi/2, pi/2), ylim = c(-1, 1))
```

With these new points, we want to train a new set of emulators. We generated 40 points from `generate_new_runs` so as to split them into a training set and a validation set once more:

```{r}
sample2d <- sample(40, 20)
train2d <- new_data2d[sample2d,]
valid2d <- new_data2d[!seq_along(new_data2d[,1])%in%sample2d,]
```

We train the new emulators:

```{r}
ems2d_2 <- emulator_from_data(train2d, c('f1', 'f2'), ranges2d)
ems2d_2
```
One thing we can see from this is that the emulators' global variances have reduced, as one would expect. We check again on the emulator diagnostics and modify as necessary:

```{r fig.width = 7, fig.height = 7}
invalid_points2 <- validation_diagnostics(ems2d_2, targets2d, valid2d)
```

We briefly look at the implausibilities for these new emulators:

```{r fig.width = 7, fig.height = 7}
emulator_plot(ems2d_2, 'imp', targets = targets2d)
```

Finally, we propose points from a combination of these wave and the previous wave.

```{r fig.width = 7, fig.height = 7}
new_new_points2d <- generate_new_runs(c(ems2d_2, ems2d), 40, c(targets2d, targets2d))
plot(data = new_new_points2d, y ~ x, xlim = c(-pi, pi), ylim = c(-1, 1), pch = 16)
```

For completeness, we record the function values of these new points:

```{r}
new_new_data2d <- data.frame(x = new_new_points2d$x, y = new_new_points2d$y,
                             f1 = apply(new_new_points2d, 1, func1), f2 = apply(new_new_points2d, 1, func2))
```

We conclude this example by looking at the evolution of the allowed space, and the relative performance of the proposed points. A variety of functions are available to do so: we first package up all our waves into a list.

```{r}
all_waves <- list(rbind(initial_data, validation_data), new_data2d, new_new_data2d)
```

We can view the output in three main ways: looking at the pure performance of the runs relative to the targets, looking at the distribution of the proposed points as the waves progress, or looking at the distribution of the output points as the waves progress. The functions `simulator_plot`, `wave_points`, and `wave_variance` do just that.

```{r fig.width = 7, fig.height = 7}
simulator_plot(all_waves, z = targets2d)
wave_points(all_waves, names(ranges2d))
wave_values(all_waves, targets2d, l_wid = 0.8)
```

*Add some more explanation here???*
We can see a clear trend, as the colour moves from light to dark, of increased performance.

### The Structure of a Bayes Linear Emulator

The basic structure of an emulator $f(x)$ is

\(f(x) = \sum_i \beta_i h_i(x) + u(x),\)

where the first term represents a regression surface (encapsulating the global behaviour of the function), and the second term accounts for local variations by defining an correlation structure on the space (more of which later). Our prior beliefs about the emulator must be specified. We need only second-order specifications (expectation and variance), so one can see that we must specifiy the following:
- A set of basis functions, $h(x)$;
- The second-order specifications for the coefficients $\beta$: namely the expectation and the variance $\mathbb{E}[\beta]$ and $\text{Var}[\beta]$;
- The second order specifications for the correlation structure: $\mathbb{E}[u(x)]$ and $\text{Var}[u(x)]$;
- The covariance between the coefficients and the correlation structure: $\text{Cov}[\beta, u(x)]$.
We could specify all of these things by hand; however, many of the parts of the above specification can be estimated quite readily automatically. The function `emulator_from_data` does exactly that, with a few assumptions. It assumes no covariance between the regression surface and the correlation structure, that the expectation of $u(x)$ is $0$, and that the variance of the coefficients $\beta$ is $0$ (i.e. that the regression surface is fixed and known); it also assumes that the correlation structure has an exponential-squared form. For two points $x$ and $x^\prime$, the correlation between them is

\(c(x,x^\prime) = \exp\left\{\frac{-(x-x^\prime)^2}{\theta^2}\right\}.\)

The closer two points are together, the higher their correlation; the parameter $\theta$ is known as a correlation length. The larger $\theta$, the larger the extent of the correlation between distant points. The `emulator_from_data` function also attempts to estimate the value of $\theta$.

### Bayes Linear Updates

The above analysis gives us a set of prior specifications for the emulator. However, it has not used all the information available from the training data. We can update our second-order beliefs with the *Bayes Linear Update Equations* - given data $D$ and prior specifications $\mathbb{E}[f(x)]$ and $\text{Var}[f(x)]$ for the emulator, the adjusted expectation and variance are

\(\mathbb{E}_D[f(x)] = \mathbb{E}[f(x)] + \text{Cov}[f(x), D]\text{Var}[D]^{-1}(D-\mathbb{E}[D]),\)
\(\text{Var}_D[f(x)] = \text{Var}[f(X)] - \text{Cov}[f(x), D]\text{Var}[D]^{-1}\text{Cov}[D, f(x)].\)

For details of the Bayes Linear Framework, see eg Bayes Linear Statistics (Goldstein & Wooff).
